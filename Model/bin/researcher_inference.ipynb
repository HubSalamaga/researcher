{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hubert\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Hubert\\anaconda3\\envs\\bs4\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['High']\n",
      "[0.7673898935317993]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import os\n",
    "\n",
    "\n",
    "# Define the regression model class (as you did before)\n",
    "class BertForRegression(torch.nn.Module):\n",
    "    def __init__(self, model_name, hidden_size=768):\n",
    "        super(BertForRegression, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.regressor = torch.nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None, return_embeddings=False):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        if return_embeddings:\n",
    "            return pooled_output  # Return embeddings directly\n",
    "        return self.regressor(pooled_output)\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove everything between angle brackets\n",
    "    clean_text = re.sub(r'<[^>]*>', '', text)\n",
    "    # Remove newline and other extra whitespace characters\n",
    "    clean_text = clean_text.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ').strip()\n",
    "    # Replace multiple spaces with a single space\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text)\n",
    "    return clean_text\n",
    "\n",
    "# Initialize the model with the same model name used during training\n",
    "model = BertForRegression(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")\n",
    "\n",
    "# Load the state dictionary\n",
    "model.load_state_dict(torch.load(r\"C:\\Users\\Hubert\\Documents\\GitHub\\researcher\\Model\\finetuned_PMBERT_regression.pth\"))\n",
    "\n",
    "# If the model was trained on a GPU and you're loading on CPU, use:\n",
    "# model.load_state_dict(torch.load(\"finetuned_PMBERT_regression.pth\", map_location=torch.device('cpu')))\n",
    "\n",
    "# Set the model to evaluation mode if you're making predictions\n",
    "model.eval()\n",
    "\n",
    "# Step 1: Read the text file\n",
    "with open(r'C:\\Users\\Hubert\\Documents\\GitHub\\researcher\\Model\\data\\new_data.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "text = clean_text(text=text)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")\n",
    "\n",
    "# Step 2: Tokenize the text\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Step 3: Convert to DataLoader\n",
    "dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'])\n",
    "dataloader = DataLoader(dataset, batch_size=1)  # Batch size of 1 since we're predicting one text\n",
    "\n",
    "# Step 4: Make Predictions\n",
    "model.eval()  # Ensure the model is in eval mode\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        b_input_ids, b_input_mask = batch\n",
    "        outputs = model(b_input_ids, b_input_mask)\n",
    "        predictions.append(outputs.squeeze().item())\n",
    "\n",
    "# Step 5: Interpret Results\n",
    "# If you are performing regression, the output is already in `predictions`\n",
    "# If you need to classify based on a threshold:\n",
    "threshold = 0.7  # Example threshold\n",
    "classified_outputs = [\"High\" if pred >= threshold else \"Low\" for pred in predictions]\n",
    "\n",
    "print(classified_outputs)\n",
    "print(predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of file 11056344_body.html:\n",
      "Introduction Neuromuscular diseases comprise a spectrum of disorders affecting motor neurons in the spinal cord, sensory neurons in the dorsal root ganglia, peripheral nerves, neuromuscular junction and/or skeletal muscles. Introduction Neuromuscular diseases comprise a spectrum of disorders affecting motor neurons in the spinal cord, sensory neurons in the dorsal root ganglia, peripheral nerves, neuromuscular junction and/or skeletal muscles. Introduction Neuromuscular diseases comprise a spectrum of disorders affecting motor neurons in the spinal cord, sensory neurons in the dorsal root ganglia, peripheral nerves, neuromuscular junction and/or skeletal muscles. Introduction Neuromuscular diseases comprise a spectrum of disorders affecting motor neurons in the spinal cord, sensory neurons in the dorsal root ganglia, peripheral nerves, neuromuscular junction and/or skeletal muscles. Introduction Neuromuscular diseases comprise a spectrum of disorders affecting motor neurons in the spinal cord, sensory neurons in the dorsal root ganglia, peripheral nerves, neuromuscular junction and/or skeletal muscles.\n",
      "\n",
      "Summary of file 11057178_body.html:\n",
      "Introduction Genome sequencing (GS) is increasingly becoming a standard genetic test for rare disease diagnosis and research [1, 2], capturing variants in both the coding and non-coding genomic space, and resulting in approximately 75,000 rare variants at ≤ 1% population allele frequency, per individual, for clinical consideration [3]. Introduction Genome sequencing (GS) is increasingly becoming a standard genetic test for rare disease diagnosis and research [1, 2], capturing variants in both the coding and non-coding genomic space, and resulting in approximately 75,000 rare variants at ≤ 1% population allele frequency, per individual, for clinical consideration [3]. Introduction Genome sequencing (GS) is increasingly becoming a standard genetic test for rare disease diagnosis and research [1, 2], capturing variants in both the coding and non-coding genomic space, and resulting in approximately 75,000 rare variants at ≤ 1% population allele frequency, per individual, for clinical consideration [3]. Introduction Genome sequencing (GS) is increasingly becoming a standard genetic test for rare disease diagnosis and research [1, 2], capturing variants in both the coding and non-coding genomic space, and resulting in approximately 75,000 rare variants at ≤ 1% population allele frequency, per individual, for clinical consideration [3]. Introduction Genome sequencing (GS) is increasingly becoming a standard genetic test for rare disease diagnosis and research [1, 2], capturing variants in both the coding and non-coding genomic space, and resulting in approximately 75,000 rare variants at ≤ 1% population allele frequency, per individual, for clinical consideration [3].\n",
      "\n",
      "Summary of file 11057403_body.html:\n",
      "Introduction In mammals, ejaculated sperm migrate toward the fertilizing site along the female reproductive tract. Introduction In mammals, ejaculated sperm migrate toward the fertilizing site along the female reproductive tract. Introduction In mammals, ejaculated sperm migrate toward the fertilizing site along the female reproductive tract. Introduction In mammals, ejaculated sperm migrate toward the fertilizing site along the female reproductive tract. Introduction In mammals, ejaculated sperm migrate toward the fertilizing site along the female reproductive tract.\n",
      "\n",
      "Summary of file 11062292_body.html:\n",
      "WEDNESDAY, MAY 8 SESSION 1–TRAUMA Room: Potomac Ballroom Moderators: Jonathan G. Schoenecker and Melinda Witbreuk OP-1 8:06 AM–8:10 AM Radiological, clinical, and functional outcome of children with traumatic hip dislocation: review of 66 cases Sara De Salvo, Shunyou Chen, Fabio Sammartino, Jeanne-Agathe Mujadiki Luesa, Yunan Lu, Wentao Wang, Liwei Shi, Lianyong Li, Vito Pavone, Federico Canavese, France-China-Italy Traumatic Hip Dislocation Study Group CHU Lille, Lille, France OP-2 8:11 AM–8:15 AM Diaphyseal femur fractures in children under the age of 3—risk factors for non-accidental trauma: a CORTICES multi-center study Manya Bali, Patricia E. Miller, Benjamin J. WEDNESDAY, MAY 8 SESSION 1–TRAUMA Room: Potomac Ballroom Moderators: Jonathan G. Schoenecker and Melinda Witbreuk OP-1 8:06 AM–8:10 AM Radiological, clinical, and functional outcome of children with traumatic hip dislocation: review of 66 cases Sara De Salvo, Shunyou Chen, Fabio Sammartino, Jeanne-Agathe Mujadiki Luesa, Yunan Lu, Wentao Wang, Liwei Shi, Lianyong Li, Vito Pavone, Federico Canavese, France-China-Italy Traumatic Hip Dislocation Study Group CHU Lille, Lille, France OP-2 8:11 AM–8:15 AM Diaphyseal femur fractures in children under the age of 3—risk factors for non-accidental trauma: a CORTICES multi-center study Manya Bali, Patricia E. Miller, Benjamin J. WEDNESDAY, MAY 8 SESSION 1–TRAUMA Room: Potomac Ballroom Moderators: Jonathan G. Schoenecker and Melinda Witbreuk OP-1 8:06 AM–8:10 AM Radiological, clinical, and functional outcome of children with traumatic hip dislocation: review of 66 cases Sara De Salvo, Shunyou Chen, Fabio Sammartino, Jeanne-Agathe Mujadiki Luesa, Yunan Lu, Wentao Wang, Liwei Shi, Lianyong Li, Vito Pavone, Federico Canavese, France-China-Italy Traumatic Hip Dislocation Study Group CHU Lille, Lille, France OP-2 8:11 AM–8:15 AM Diaphyseal femur fractures in children under the age of 3—risk factors for non-accidental trauma: a CORTICES multi-center study Manya Bali, Patricia E. Miller, Benjamin J. WEDNESDAY, MAY 8 SESSION 1–TRAUMA Room: Potomac Ballroom Moderators: Jonathan G. Schoenecker and Melinda Witbreuk OP-1 8:06 AM–8:10 AM Radiological, clinical, and functional outcome of children with traumatic hip dislocation: review of 66 cases Sara De Salvo, Shunyou Chen, Fabio Sammartino, Jeanne-Agathe Mujadiki Luesa, Yunan Lu, Wentao Wang, Liwei Shi, Lianyong Li, Vito Pavone, Federico Canavese, France-China-Italy Traumatic Hip Dislocation Study Group CHU Lille, Lille, France OP-2 8:11 AM–8:15 AM Diaphyseal femur fractures in children under the age of 3—risk factors for non-accidental trauma: a CORTICES multi-center study Manya Bali, Patricia E. Miller, Benjamin J. WEDNESDAY, MAY 8 SESSION 1–TRAUMA Room: Potomac Ballroom Moderators: Jonathan G. Schoenecker and Melinda Witbreuk OP-1 8:06 AM–8:10 AM Radiological, clinical, and functional outcome of children with traumatic hip dislocation: review of 66 cases Sara De Salvo, Shunyou Chen, Fabio Sammartino, Jeanne-Agathe Mujadiki Luesa, Yunan Lu, Wentao Wang, Liwei Shi, Lianyong Li, Vito Pavone, Federico Canavese, France-China-Italy Traumatic Hip Dislocation Study Group CHU Lille, Lille, France OP-2 8:11 AM–8:15 AM Diaphyseal femur fractures in children under the age of 3—risk factors for non-accidental trauma: a CORTICES multi-center study Manya Bali, Patricia E. Miller, Benjamin J.\n",
      "\n",
      "Summary of file 11068800_body.html:\n",
      "Introduction The development of germ cells and their differentiation into gametes is crucial for the faithful transmission of both genetic and epigenetic information to the next generation. Introduction The development of germ cells and their differentiation into gametes is crucial for the faithful transmission of both genetic and epigenetic information to the next generation. Introduction The development of germ cells and their differentiation into gametes is crucial for the faithful transmission of both genetic and epigenetic information to the next generation. Introduction The development of germ cells and their differentiation into gametes is crucial for the faithful transmission of both genetic and epigenetic information to the next generation. Introduction The development of germ cells and their differentiation into gametes is crucial for the faithful transmission of both genetic and epigenetic information to the next generation.\n",
      "\n",
      "Summary of file 11070107_body.html:\n",
      "Background The issue of antibiotic resistance poses a significant challenge to world health. Background The issue of antibiotic resistance poses a significant challenge to world health. Background The issue of antibiotic resistance poses a significant challenge to world health. The database VaxiJen provides a reliable way to determine the antigenic characteristics of proteins. Background The issue of antibiotic resistance poses a significant challenge to world health.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")\n",
    "\n",
    "# Define the regression model class\n",
    "class BertForRegression(nn.Module):\n",
    "    def __init__(self, model_name, hidden_size=768):\n",
    "        super(BertForRegression, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.regressor = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None, return_embeddings=False):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        if return_embeddings:\n",
    "            return pooled_output  # Return embeddings directly\n",
    "        return self.regressor(pooled_output)\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove everything between angle brackets\n",
    "    clean_text = re.sub(r'<[^>]*>', '', text)\n",
    "    # Remove newline and other extra whitespace characters\n",
    "    clean_text = clean_text.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ').strip()\n",
    "    # Replace multiple spaces with a single space\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text)\n",
    "    return clean_text\n",
    "\n",
    "def get_embeddings(text, model, tokenizer, device):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "    # Use the model to get embeddings\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        embeddings = model(input_ids=input_ids, attention_mask=attention_mask, return_embeddings=True)\n",
    "    \n",
    "    return embeddings.cpu().detach().numpy()  # Convert PyTorch tensor to NumPy array for further processing\n",
    "\n",
    "def extractive_summarization(text, model, tokenizer, device, num_sentences=5):\n",
    "    sentences = sent_tokenize(text)\n",
    "    sentence_embeddings = np.vstack([get_embeddings(sent, model, tokenizer, device)[0].mean(axis=0) for sent in sentences])\n",
    "    # Clustering sentences\n",
    "    num_clusters = min(num_sentences, len(sentences))  # Ensuring we don't exceed the number of sentences\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(sentence_embeddings)\n",
    "    centroids = kmeans.cluster_centers_\n",
    "\n",
    "    # Selecting one sentence per cluster (closest to centroid)\n",
    "    summarized_sentences = []\n",
    "    for centroid in centroids:\n",
    "        similarities = cosine_similarity([centroid], sentence_embeddings)\n",
    "        best_sentence = np.argmax(similarities)\n",
    "        summarized_sentences.append(sentences[best_sentence])\n",
    "\n",
    "    return \" \".join(summarized_sentences)\n",
    "\n",
    "def process_file(file_path, model, tokenizer, device):\n",
    "    with open(file_path, \"r\", encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    text = clean_text(text)\n",
    "    summary = extractive_summarization(text, model, tokenizer, device, num_sentences=5)\n",
    "    return summary\n",
    "\n",
    "def extract_summaries_from_articles_in_dir(directory_path, model, tokenizer, device):\n",
    "    summaries = {}\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {\n",
    "            executor.submit(process_file, os.path.join(directory_path, filename), model, tokenizer, device): filename\n",
    "            for filename in os.listdir(directory_path)\n",
    "            if os.path.isfile(os.path.join(directory_path, filename))\n",
    "        }\n",
    "        for future in futures:\n",
    "            filename = futures[future]\n",
    "            summaries[filename] = future.result()\n",
    "    \n",
    "    return summaries\n",
    "\n",
    "# Initialize the model with the same model name used during training\n",
    "model = BertForRegression(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")\n",
    "model.load_state_dict(torch.load(r\"C:\\Users\\Hubert\\Documents\\GitHub\\researcher\\Model\\finetuned_PMBERT_regression.pth\"))\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Directory containing text files\n",
    "directory_path = r\"C:\\Users\\Hubert\\Documents\\GitHub\\researcher\\Model\\data\\test_articles\"\n",
    "\n",
    "# Process all files in the directory and get summaries\n",
    "summaries = extract_summaries_from_articles_in_dir(directory_path, model, tokenizer, device)\n",
    "\n",
    "# Print summaries\n",
    "for filename, summary in summaries.items():\n",
    "    print(f\"Summary of file {filename}:\\n{summary}\\n\")\n",
    "\n",
    "#IDK WHY BUT SOMETIMES ITS SUPER FAST BUT OTHERS IT TAKES FUCKING FOREVER\n",
    "# MAYBE BECAUSE I HAVE CHROME SPOTIFY RUNNING AND A MOVIE STREAMING BUT PROBABLY CHAT GPT GAVE US SHIT CODE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA version: 11.8\n",
      "Number of GPUs: 1\n",
      "Current CUDA device: 0\n",
      "GPU Name: NVIDIA GeForce GTX 1650\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bs4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
