{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hubert\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertTokenizer, BertModel, AdamW\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "from scipy.stats import expon, loguniform, uniform\n",
    "import os\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>AwT score</th>\n",
       "      <th>SoE score</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nutritional status and gene polymorphisms of o...</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.20</td>\n",
       "      <td>PMC9569987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thrombophilic gene polymorphism is known to be...</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.50</td>\n",
       "      <td>PMC6045916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Background Whether adiponectin (ADIPOQ) polymo...</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.00</td>\n",
       "      <td>PMC6278103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Polycystic ovary syndrome (PCOS) is a common, ...</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.00</td>\n",
       "      <td>PMC4557132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Objective: Endometriosis has been considered a...</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.55</td>\n",
       "      <td>21429654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Objective To present the development of the fi...</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.55</td>\n",
       "      <td>PMC7169920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Introduction: The aim of the study was to eval...</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.60</td>\n",
       "      <td>28819944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Background: Although the precise pathophysiolo...</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.00</td>\n",
       "      <td>18277167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Background Key reactions in folate-mediated si...</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.40</td>\n",
       "      <td>PMC8792379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Epidemiological studies have suggested that th...</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.60</td>\n",
       "      <td>PMID: 25102261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Aim To investigate association of factor V Lei...</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.55</td>\n",
       "      <td>PMID: 29703881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Objective: The etiology of polycystic ovarian ...</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.00</td>\n",
       "      <td>31122534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Kallmann’s syndrome (KS) is characterized by t...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.40</td>\n",
       "      <td>12050219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>We sought to verify whether variation in the p...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.55</td>\n",
       "      <td>17339269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Context: Prior studies showed that Axl /Tyro3 ...</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.50</td>\n",
       "      <td>PMC3973777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text   AwT score   SoE score  \\\n",
       "0   Nutritional status and gene polymorphisms of o...        0.90        0.20   \n",
       "1   Thrombophilic gene polymorphism is known to be...        0.90        0.50   \n",
       "2   Background Whether adiponectin (ADIPOQ) polymo...        0.95        1.00   \n",
       "3   Polycystic ovary syndrome (PCOS) is a common, ...        0.95        1.00   \n",
       "4   Objective: Endometriosis has been considered a...        0.95        0.55   \n",
       "5   Objective To present the development of the fi...        0.95        0.55   \n",
       "6   Introduction: The aim of the study was to eval...        0.95        0.60   \n",
       "7   Background: Although the precise pathophysiolo...        0.95        1.00   \n",
       "8   Background Key reactions in folate-mediated si...        0.95        0.40   \n",
       "9   Epidemiological studies have suggested that th...        0.95        0.60   \n",
       "10  Aim To investigate association of factor V Lei...        0.95        0.55   \n",
       "11  Objective: The etiology of polycystic ovarian ...        0.95        1.00   \n",
       "12  Kallmann’s syndrome (KS) is characterized by t...        0.75        0.40   \n",
       "13  We sought to verify whether variation in the p...        0.75        0.55   \n",
       "14  Context: Prior studies showed that Axl /Tyro3 ...        0.85        0.50   \n",
       "\n",
       "                 ID  \n",
       "0        PMC9569987  \n",
       "1        PMC6045916  \n",
       "2        PMC6278103  \n",
       "3        PMC4557132  \n",
       "4          21429654  \n",
       "5        PMC7169920  \n",
       "6          28819944  \n",
       "7          18277167  \n",
       "8        PMC8792379  \n",
       "9    PMID: 25102261  \n",
       "10   PMID: 29703881  \n",
       "11         31122534  \n",
       "12         12050219  \n",
       "13         17339269  \n",
       "14       PMC3973777  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data_from_csv(file_path):\n",
    "    dataframe = pd.read_csv(file_path)\n",
    "    return dataframe\n",
    "\n",
    "cwd = os.getcwd()\n",
    "file_path = os.path.join(cwd, r\"data\\initial_training_data\\new_data.csv\")\n",
    "dataset = load_data_from_csv(file_path)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_scores(scores, max_deviation= 0.05):\n",
    "    randomized_scores = scores * (1 + np.random.uniform(-max_deviation,max_deviation,size=scores.shape))\n",
    "    return np.clip(randomized_scores,0,1)\n",
    "\n",
    "train_y = dataset[[\"AwT score\", \"SoE score\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penalty_function_AwT(AwT, alpha=2, epsilon=1e-6):\n",
    "    return torch.exp(-(AwT + epsilon)**alpha)\n",
    "\n",
    "def reward_function_AwT(AwT, beta=2):\n",
    "    return (torch.exp(AwT - 0.5))**beta\n",
    "\n",
    "def penalty_function_SoE(SoE, gamma=2, epsilon=1e-6):\n",
    "    return torch.exp(-(SoE + epsilon)**gamma)\n",
    "\n",
    "def reward_function_SoE(SoE, delta=2):\n",
    "    return (torch.exp(SoE - 0.5))**delta\n",
    "\n",
    "def calculate_final_score(SoE, AwT, w_SoE=0.4, w_AwT=0.6, alpha=2, beta=2, gamma=2, delta=2):\n",
    "    base_score = w_SoE * SoE + w_AwT * AwT\n",
    "\n",
    "    AwT_score = torch.where(AwT < 0.5, base_score * penalty_function_AwT(AwT, alpha), base_score * reward_function_AwT(AwT, beta))\n",
    "    final_score = torch.where(SoE < 0.5, AwT_score * penalty_function_SoE(SoE, gamma), AwT_score * reward_function_SoE(SoE, delta))\n",
    "    \n",
    "    return final_score\n",
    "\n",
    "def custom_scorer(y_true, y_pred):\n",
    "    scores = []\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        score = calculate_final_score(true[1], true[0])  # Assuming y_true contains [AwT, SoE]\n",
    "        scores.append(score)\n",
    "    return mean_squared_error(y_true, scores)\n",
    "\n",
    "def evaluate_hyperparameters(alpha, beta, gamma, delta):\n",
    "    def model_evaluation(SoE, AwT):\n",
    "        return calculate_final_score(SoE, AwT, alpha=alpha, beta=beta, gamma=gamma, delta=delta)\n",
    "    return model_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hubert\\anaconda3\\envs\\bs4\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Hubert\\anaconda3\\envs\\bs4\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Hubert\\anaconda3\\envs\\bs4\\Lib\\site-packages\\transformers\\optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "C:\\Users\\Hubert\\AppData\\Local\\Temp\\ipykernel_3868\\1806022673.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  reward_punishment_term = torch.tensor(final_scores, dtype=torch.float32, device=predictions.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: -2.6905455589294434\n",
      "Epoch 2, Loss: -3.08303165435791\n",
      "Epoch 3, Loss: -3.2622413635253906\n",
      "Epoch 4, Loss: -3.287731885910034\n",
      "Epoch 1, Loss: -2.8857524394989014\n",
      "Epoch 2, Loss: -3.187039852142334\n",
      "Epoch 3, Loss: -3.268559455871582\n",
      "Epoch 4, Loss: -3.2718114852905273\n"
     ]
    }
   ],
   "source": [
    "# Load your dataset\n",
    "# dataset = pd.read_csv(\"path_to_your_dataset.csv\")\n",
    "# Assuming 'text' column contains abstracts and 'score' column contains labels\n",
    "\n",
    "def custom_loss(predictions, targets, w_SoE=0.4, w_AwT=0.6, alpha=2, beta=2, gamma=2, delta=2):\n",
    "    mse_loss = F.mse_loss(predictions, targets)\n",
    "    \n",
    "    AwT = targets[:, 0]\n",
    "    SoE = targets[:, 1]\n",
    "    final_scores = calculate_final_score(SoE, AwT, w_SoE, w_AwT, alpha, beta, gamma, delta)\n",
    "    \n",
    "    reward_punishment_term = torch.tensor(final_scores, dtype=torch.float32, device=predictions.device)\n",
    "    \n",
    "    # Integrate the reward/punishment term with the MSE loss\n",
    "    total_loss = mse_loss - reward_punishment_term.mean()\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "scores_to_randomize = dataset[[\"AwT score\", \"SoE score\"]].values\n",
    "randomized_scores = randomize_scores(scores_to_randomize)\n",
    "dataset[[\"AwT score\", \"SoE score\"]] = randomized_scores\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.4, random_state=42)\n",
    "\n",
    "# Initializing tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")\n",
    "\n",
    "# Function to prepare DataLoader\n",
    "def prepare_dataloader(data, batch_size=6, test=False):\n",
    "    inputs = tokenizer(data[\"text\"].tolist(), padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    labels = torch.tensor(data[[\"AwT score\", \"SoE score\"]].values).float()\n",
    "    dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'], labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    return dataloader\n",
    "\n",
    "train_dataloader = prepare_dataloader(train_data,batch_size=15)\n",
    "\n",
    "class BertForRegression(nn.Module):\n",
    "    def __init__(self, model_name, hidden_size=768):\n",
    "        super(BertForRegression, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.regressor = nn.Linear(hidden_size, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None, return_embeddings=False):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        if return_embeddings:\n",
    "            return pooled_output\n",
    "        return self.regressor(pooled_output)\n",
    "    \n",
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "def train_model(train_dataloader, device, epochs = 4, model_index = 0):\n",
    "    model = BertForRegression(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")\n",
    "    model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5) # test value  # torch.optim.AdamW\n",
    "    #criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            b_input_ids, b_input_mask, b_labels = [item.to(device) for item in batch] #co to jest b_input_ids\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(b_input_ids,b_input_mask) #co to jest????\n",
    "            #loss = criterion(outputs.squeeze(),b_labels)\n",
    "            loss = custom_loss(outputs, b_labels)  # Use the custom loss function #outputs = predictions , b_labels = targets \n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        model_index += 1\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {avg_train_loss}\")\n",
    "        save_model(model, f\"trained_model_{model_index}.pt\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "num_models = 2\n",
    "models = [train_model(train_dataloader, device) for _ in range(num_models)]\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_models(models, test_dataloader, device):\n",
    "    all_predictions = []\n",
    "    true_scores = []\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for batch in test_dataloader:\n",
    "                b_input_ids, b_input_mask, b_labels = [item.to(device) for item in batch]\n",
    "                outputs = model(b_input_ids, b_input_mask)\n",
    "                predictions.extend(outputs.squeeze().cpu().numpy())\n",
    "                true_scores.extend(b_labels.cpu().numpy())\n",
    "        all_predictions.append(predictions)\n",
    "    avg_predictions = np.mean(all_predictions, axis=0)\n",
    "    true_scores = np.array(true_scores)\n",
    "    # Calculate custom scores\n",
    "    SoE_scores = true_scores[:, 1]\n",
    "    AwT_scores = true_scores[:, 0]\n",
    "    final_scores = []\n",
    "    for i in range(len(avg_predictions)):\n",
    "        final_score = calculate_final_score(avg_predictions[i][1], avg_predictions[i][0])\n",
    "        final_scores.append(final_score)\n",
    "    \n",
    "    final_scores = np.array(final_scores)\n",
    "    return avg_predictions, all_predictions, final_scores\n",
    "\n",
    "test_dataloader = prepare_dataloader(test_data, batch_size=15, test=True) # Use your actual test data here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(models, test_dataloader, device):\n",
    "    all_predictions = []\n",
    "    true_scores = []\n",
    "\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for batch in test_dataloader:\n",
    "                b_input_ids, b_input_mask, b_labels = [item.to(device) for item in batch]\n",
    "                outputs = model(b_input_ids, b_input_mask)\n",
    "                predictions.extend(outputs.cpu().numpy())\n",
    "                true_scores.extend(b_labels.cpu().numpy())\n",
    "        all_predictions.append(predictions)\n",
    "\n",
    "    avg_predictions = np.mean(all_predictions, axis=0)\n",
    "    true_scores = np.array(true_scores)\n",
    "\n",
    "    # Calculate custom scores\n",
    "    SoE_scores = true_scores[:, 1]\n",
    "    AwT_scores = true_scores[:, 0]\n",
    "    \n",
    "    final_scores = calculate_final_score(\n",
    "        torch.tensor(SoE_scores, device=device),\n",
    "        torch.tensor(AwT_scores, device=device),\n",
    "        w_SoE=0.4, w_AwT=0.6, alpha=2, beta=2, gamma=2, delta=2\n",
    "    ).cpu().numpy()\n",
    "\n",
    "    return avg_predictions, all_predictions, final_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['AwT score', 'SoE score'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 17\u001b[0m\n\u001b[0;32m      1\u001b[0m param_distributions \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m : [expon(scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m), uniform(\u001b[38;5;241m0.1\u001b[39m,\u001b[38;5;241m1.9\u001b[39m)],\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeta\u001b[39m\u001b[38;5;124m'\u001b[39m : [loguniform(\u001b[38;5;241m1e-3\u001b[39m,\u001b[38;5;241m1e1\u001b[39m),uniform(\u001b[38;5;241m0.1\u001b[39m,\u001b[38;5;241m1.9\u001b[39m)],\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m'\u001b[39m : [expon(scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m),uniform(\u001b[38;5;241m0.1\u001b[39m,\u001b[38;5;241m1.9\u001b[39m)],\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m'\u001b[39m : [uniform(\u001b[38;5;241m0.1\u001b[39m,\u001b[38;5;241m1.9\u001b[39m), expon(scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)]\n\u001b[0;32m      6\u001b[0m }\n\u001b[0;32m      8\u001b[0m random_search \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(\n\u001b[0;32m      9\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mevaluate_hyperparameters,\n\u001b[0;32m     10\u001b[0m     param_distributions\u001b[38;5;241m=\u001b[39mparam_distributions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[0;32m     15\u001b[0m )\n\u001b[1;32m---> 17\u001b[0m x_train \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAwT score\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSoE score\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     18\u001b[0m y_train \u001b[38;5;241m=\u001b[39m dataset[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAwT score\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSoE score\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     20\u001b[0m random_search\u001b[38;5;241m.\u001b[39mfit(x_train, y_train)\n",
      "File \u001b[1;32mc:\\Users\\Hubert\\anaconda3\\envs\\bs4\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Hubert\\anaconda3\\envs\\bs4\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Hubert\\anaconda3\\envs\\bs4\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[0;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[1;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['AwT score', 'SoE score'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "param_distributions = {\n",
    "    'alpha' : [expon(scale=1.0), uniform(0.1,1.9)],\n",
    "    'beta' : [loguniform(1e-3,1e1),uniform(0.1,1.9)],\n",
    "    'gamma' : [expon(scale=1.0),uniform(0.1,1.9)],\n",
    "    'delta' : [uniform(0.1,1.9), expon(scale=1.0)]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=evaluate_hyperparameters,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=100,\n",
    "    scoring=make_scorer(custom_scorer, greater_is_better=True),\n",
    "    cv=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "x_train = dataset[[\"AwT score\", \"SoE score\"]].values\n",
    "y_train = dataset[[\"AwT score\", \"SoE score\"]].values\n",
    "\n",
    "random_search.fit(x_train, y_train)\n",
    "best_params = random_search.best_params_\n",
    "print(f\"Best parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    AwT score  SoE score\n",
      "0    0.905837   0.204037\n",
      "5    0.975222   0.538041\n",
      "8    0.917598   0.415331\n",
      "9    0.928707   0.580388\n",
      "11   0.961822   1.000000\n",
      "13   0.737319   0.541878\n",
      "Shape of avg_predictions: (6, 2)\n",
      "Shape of true_scores: (6, 2)\n",
      "Average MSE: AwT score    0.032119\n",
      "SoE score    0.125733\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "avg_predictions, all_predictions, final_score = evaluate_models(models, test_dataloader, device)\n",
    "# Assuming true_scores is already defined correctly\n",
    "true_scores = dataset.drop(test_data.index)\n",
    "train_scores_true = dataset.drop(true_scores.index)\n",
    "train_scores_true = train_scores_true.iloc[:,1:3]\n",
    "print(train_scores_true)\n",
    "#Check the shape of avg_predictions\n",
    "print(f\"Shape of avg_predictions: {avg_predictions.shape}\")\n",
    "print(f\"Shape of true_scores: {train_scores_true.shape}\")\n",
    "\n",
    "mse = ((avg_predictions - train_scores_true) ** 2).mean(axis=0)\n",
    "print(f\"Average MSE: {mse}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.98249406, 0.8352511 ],\n",
       "       [1.0636257 , 0.81183314],\n",
       "       [1.0792396 , 0.7447859 ],\n",
       "       [1.0759197 , 0.7246385 ],\n",
       "       [1.063615  , 0.7327546 ],\n",
       "       [1.0849732 , 0.825143  ]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bs4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
