{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertTokenizer, BertModel, AdamW\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "from scipy.stats import expon, loguniform, uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_scores(scores, max_deviation= 0.05):\n",
    "    randomize_scores = scores * (1 + np.random.uniform(-max_deviation,max_deviation,size=scores.shape))\n",
    "    return np.clip(randomize_scores,0,1)\n",
    "\n",
    "\n",
    "#This will be a separate module\n",
    "def penalty_function_AwT(AwT, alpha = 1, epsilon = 1e-6):\n",
    "    if AwT < 0.5:\n",
    "        return 1 / (AwT+epsilon)**alpha\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def reward_function_AwT(AwT, beta = 1):\n",
    "    if AwT > 0.5:\n",
    "        return (np.exp(AwT - 0.5))**beta\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def penalty_function_SoE(SoE, gamma=0.5, epsilon=1e-6):\n",
    "    if SoE < 0.5:\n",
    "        return 1 / (SoE + epsilon)**gamma\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def reward_function_SoE(SoE, delta=0.5):\n",
    "    if SoE > 0.5:\n",
    "        return (np.exp(SoE - 0.5))**delta\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def calculate_final_score(SoE, AwT, w_SoE=0.4, w_AwT=0.6, alpha=1, beta=1, gamma=0.5, delta=0.5):\n",
    "    base_score = w_SoE * SoE + w_AwT * AwT\n",
    "    \n",
    "    if AwT < 0.5:\n",
    "        AwT_score = base_score * penalty_function_AwT(AwT, alpha)\n",
    "    else:\n",
    "        AwT_score = base_score * reward_function_AwT(AwT, beta)\n",
    "    \n",
    "    if SoE < 0.5:\n",
    "        final_score = AwT_score * penalty_function_SoE(SoE, gamma)\n",
    "    else:\n",
    "        final_score = AwT_score * reward_function_SoE(SoE, delta)\n",
    "    \n",
    "    return final_score\n",
    "\n",
    "def custom_scorer(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred)\n",
    "\n",
    "def evaluate_model(alpha, beta, gamma, delta):\n",
    "    def model_evaluation(SoE, AwT):\n",
    "        return calculate_final_score(SoE, AwT, alpha=alpha, beta=beta, gamma=gamma, delta=delta)\n",
    "    return model_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "# dataset = pd.read_csv(\"path_to_your_dataset.csv\")\n",
    "# Assuming 'text' column contains abstracts and 'score' column contains labels\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Splitting dataset\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "test_data[\"randomized_score\"] = randomize_scores(test_data[\"score\"].values)\n",
    "train_data[\"randomized_score\"] = randomize_scores(train_data[\"score\"].values)\n",
    "\n",
    "# Initializing tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")\n",
    "\n",
    "# Function to prepare DataLoader\n",
    "def prepare_dataloader(data, score_column=\"randomized_score\", batch_size=6):\n",
    "    inputs = tokenizer(data[\"text\"].tolist(), padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    labels = torch.tensor(data[score_column].tolist()).float()\n",
    "    dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'], labels) #?????\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader\n",
    "\n",
    "train_dataloader = prepare_dataloader(train_data)\n",
    "test_dataloader = prepare_dataloader(test_data, score_column=\"score\", batch_size=1)\n",
    "\n",
    "\n",
    "\n",
    "class BertForRegression(nn.Module):\n",
    "    def __init__(self, model_name, hidden_size=768):\n",
    "        super(BertForRegression, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.regressor = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None, return_embeddings=False):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        if return_embeddings:\n",
    "            return pooled_output\n",
    "        return self.regressor(pooled_output)\n",
    "    \n",
    "\n",
    "\n",
    "def train_model(train_dataloader, device, epochs = 16):\n",
    "    model = BertTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")\n",
    "    model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5) # test value \n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            b_input_ids, b_input_mask, b_labels = [item.to(device) for item in batch]\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(b_input_ids,b_input_mask)\n",
    "            loss = criterion(outputs.squeeze(),b_labels)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {avg_train_loss}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_models = 5\n",
    "models = [train_model(train_dataloader, device) for _ in range(num_models)]\n",
    "\n",
    "def evaluate_models(models, test_dataloader, device):\n",
    "    all_predictions = []\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for batch in test_dataloader:\n",
    "                b_input_ids, b_input_mask, _ = [item.to(device) for item in batch]\n",
    "                outputs = model(b_input_ids, b_input_mask)\n",
    "                predictions.extend(outputs.squeeze().cpu().numpy())\n",
    "        all_predictions.append(predictions)\n",
    "    \n",
    "    avg_predictions = np.mean(all_predictions, axis=0)\n",
    "    return avg_predictions\n",
    "\n",
    "avg_predictions = evaluate_models(models, test_dataloader, device)\n",
    "mse = ((avg_predictions - test_data[\"score\"].values) ** 2).mean()\n",
    "print(f\"Average MSE: {mse}\")\n",
    "\n",
    "\n",
    "param_distributions = {\n",
    "    'alpha' : [expon(scale=1.0), uniform(0.1,1.9)],\n",
    "    'beta' : [loguniform(1e-3,1e1),uniform(0.1,1.9)],\n",
    "    'gamma' : [expon(scale=1.0),uniform(0.1,1.9)],\n",
    "    'delta' : [uniform(0.1,1.9), expon(scale=1.0)]\n",
    "}\n",
    "\n",
    "random_search=RandomizedSearchCV(\n",
    "    estimator=evaluate_model,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=100,\n",
    "    scoring=make_scorer(custom_scorer, greater_is_better= False),\n",
    "    cv=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Example data\n",
    "# X_train, y_train should be your training data\n",
    "# x_train: array of pairs (SoE, AwT)\n",
    "# y_train: corresponding true scores\n",
    "\n",
    "random_search.fit(x_train,y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bs4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
