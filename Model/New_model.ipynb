{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertTokenizer, BertModel, AdamW\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "from scipy.stats import expon, loguniform, uniform\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>AwT score</th>\n",
       "      <th>SoE score</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nutritional status and gene polymorphisms of o...</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.20</td>\n",
       "      <td>PMC9569987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thrombophilic gene polymorphism is known to be...</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.50</td>\n",
       "      <td>PMC6045916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Background Whether adiponectin (ADIPOQ) polymo...</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.00</td>\n",
       "      <td>PMC6278103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Polycystic ovary syndrome (PCOS) is a common, ...</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.00</td>\n",
       "      <td>PMC4557132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Objective: Endometriosis has been considered a...</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.55</td>\n",
       "      <td>21429654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Objective To present the development of the fi...</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.55</td>\n",
       "      <td>PMC7169920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Introduction: The aim of the study was to eval...</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.60</td>\n",
       "      <td>28819944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Background: Although the precise pathophysiolo...</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.00</td>\n",
       "      <td>18277167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Background Key reactions in folate-mediated si...</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.40</td>\n",
       "      <td>PMC8792379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Epidemiological studies have suggested that th...</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.60</td>\n",
       "      <td>PMID: 25102261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Aim To investigate association of factor V Lei...</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.55</td>\n",
       "      <td>PMID: 29703881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Objective: The etiology of polycystic ovarian ...</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.00</td>\n",
       "      <td>31122534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Kallmann’s syndrome (KS) is characterized by t...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.40</td>\n",
       "      <td>12050219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>We sought to verify whether variation in the p...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.55</td>\n",
       "      <td>17339269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Context: Prior studies showed that Axl /Tyro3 ...</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.50</td>\n",
       "      <td>PMC3973777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text   AwT score   SoE score  \\\n",
       "0   Nutritional status and gene polymorphisms of o...        0.90        0.20   \n",
       "1   Thrombophilic gene polymorphism is known to be...        0.90        0.50   \n",
       "2   Background Whether adiponectin (ADIPOQ) polymo...        0.95        1.00   \n",
       "3   Polycystic ovary syndrome (PCOS) is a common, ...        0.95        1.00   \n",
       "4   Objective: Endometriosis has been considered a...        0.95        0.55   \n",
       "5   Objective To present the development of the fi...        0.95        0.55   \n",
       "6   Introduction: The aim of the study was to eval...        0.95        0.60   \n",
       "7   Background: Although the precise pathophysiolo...        0.95        1.00   \n",
       "8   Background Key reactions in folate-mediated si...        0.95        0.40   \n",
       "9   Epidemiological studies have suggested that th...        0.95        0.60   \n",
       "10  Aim To investigate association of factor V Lei...        0.95        0.55   \n",
       "11  Objective: The etiology of polycystic ovarian ...        0.95        1.00   \n",
       "12  Kallmann’s syndrome (KS) is characterized by t...        0.75        0.40   \n",
       "13  We sought to verify whether variation in the p...        0.75        0.55   \n",
       "14  Context: Prior studies showed that Axl /Tyro3 ...        0.85        0.50   \n",
       "\n",
       "                 ID  \n",
       "0        PMC9569987  \n",
       "1        PMC6045916  \n",
       "2        PMC6278103  \n",
       "3        PMC4557132  \n",
       "4          21429654  \n",
       "5        PMC7169920  \n",
       "6          28819944  \n",
       "7          18277167  \n",
       "8        PMC8792379  \n",
       "9    PMID: 25102261  \n",
       "10   PMID: 29703881  \n",
       "11         31122534  \n",
       "12         12050219  \n",
       "13         17339269  \n",
       "14       PMC3973777  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data_from_csv(file_path):\n",
    "    dataframe = pd.read_csv(file_path)\n",
    "    return dataframe\n",
    "\n",
    "cwd = os.getcwd()\n",
    "file_path = os.path.join(cwd, r\"data\\initial_training_data\\new_data.csv\")\n",
    "dataset = load_data_from_csv(file_path)\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9  0.2 ]\n",
      " [0.9  0.5 ]\n",
      " [0.95 1.  ]\n",
      " [0.95 1.  ]\n",
      " [0.95 0.55]\n",
      " [0.95 0.55]\n",
      " [0.95 0.6 ]\n",
      " [0.95 1.  ]\n",
      " [0.95 0.4 ]\n",
      " [0.95 0.6 ]\n",
      " [0.95 0.55]\n",
      " [0.95 1.  ]\n",
      " [0.75 0.4 ]\n",
      " [0.75 0.55]\n",
      " [0.85 0.5 ]]\n",
      "[[0.9173911  0.20410323]\n",
      " [0.89328658 0.50586134]\n",
      " [0.96121359 1.        ]\n",
      " [0.99524756 1.        ]\n",
      " [0.93993131 0.56981727]\n",
      " [0.91885347 0.56277266]\n",
      " [0.93643383 0.60526664]\n",
      " [0.92387992 1.        ]\n",
      " [0.98821685 0.3994998 ]\n",
      " [0.94048028 0.60594862]\n",
      " [0.9102192  0.52372851]\n",
      " [0.97351655 0.99271368]\n",
      " [0.72334145 0.41483661]\n",
      " [0.77450697 0.54330418]\n",
      " [0.86813289 0.50324535]]\n"
     ]
    }
   ],
   "source": [
    "def randomize_scores(scores, max_deviation= 0.05):\n",
    "    randomized_scores = scores * (1 + np.random.uniform(-max_deviation,max_deviation,size=scores.shape))\n",
    "    return np.clip(randomized_scores,0,1)\n",
    "\n",
    "train_x = dataset[\"text\"].tolist()\n",
    "train_y = dataset[[\" AwT score\", \" SoE score\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1622302265963973\n",
      "2.117000016612675\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#This will be a separate module\n",
    "def penalty_function_AwT(AwT, alpha = 1, epsilon = 1e-6):\n",
    "    if AwT < 0.5:\n",
    "        return 1 / (AwT+epsilon)**alpha\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def reward_function_AwT(AwT, beta = 1):\n",
    "    if AwT > 0.5:\n",
    "        return (np.exp(AwT - 0.5))**beta\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def penalty_function_SoE(SoE, gamma=0.5, epsilon=1e-6):\n",
    "    if SoE < 0.5:\n",
    "        return 1 / (SoE + epsilon)**gamma\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def reward_function_SoE(SoE, delta=0.5):\n",
    "    if SoE > 0.5:\n",
    "        return (np.exp(SoE - 0.5))**delta\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def calculate_final_score(SoE, AwT, w_SoE=0.4, w_AwT=0.6, alpha=1, beta=1, gamma=0.5, delta=0.5):\n",
    "    base_score = w_SoE * SoE + w_AwT * AwT\n",
    "    \n",
    "    if AwT < 0.5:\n",
    "        AwT_score = base_score * penalty_function_AwT(AwT, alpha)\n",
    "    else:\n",
    "        AwT_score = base_score * reward_function_AwT(AwT, beta)\n",
    "    \n",
    "    if SoE < 0.5:\n",
    "        final_score = AwT_score * penalty_function_SoE(SoE, gamma)\n",
    "    else:\n",
    "        final_score = AwT_score * reward_function_SoE(SoE, delta)\n",
    "    \n",
    "    return final_score\n",
    "\n",
    "def custom_scorer(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred)\n",
    "\n",
    "def evaluate_hyperparameters(alpha, beta, gamma, delta):\n",
    "    def model_evaluation(SoE, AwT):\n",
    "        return calculate_final_score(SoE, AwT, alpha=alpha, beta=beta, gamma=gamma, delta=delta)\n",
    "    return model_evaluation\n",
    "\n",
    "\n",
    "print(calculate_final_score(SoE=0.1,AwT=0.1))\n",
    "print(calculate_final_score(SoE=1,AwT=1))\n",
    "print(\"not what we wanted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.039878387368098844\n",
      "0.06595764114738317\n",
      "0.08660329345586115\n",
      "the closer to 0 the stronger penalty\n",
      "/n 0.3031426331155356 0.5 0.5308134948381182 /n\n",
      "0.039878387368098844\n",
      "0.20954084990611643\n",
      "the closer to 1 the stronger reward\n",
      "7.389056098930652\n"
     ]
    }
   ],
   "source": [
    "def penalty_function_AwT(AwT, alpha=2, epsilon=1e-6):\n",
    "    return np.exp(-(AwT + epsilon)**alpha)\n",
    "\n",
    "def reward_function_AwT(AwT, beta=2):\n",
    "    return (np.exp(AwT - 0.5))**beta\n",
    "\n",
    "def penalty_function_SoE(SoE, gamma=2, epsilon=1e-6):\n",
    "    return np.exp(-(SoE + epsilon)**gamma)\n",
    "\n",
    "def reward_function_SoE(SoE, delta=2):\n",
    "    return (np.exp(SoE - 0.5))**delta\n",
    "\n",
    "def calculate_final_score(SoE, AwT, w_SoE=0.4, w_AwT=0.6, alpha=2, beta=2, gamma=2, delta=2):\n",
    "    base_score = w_SoE * SoE + w_AwT * AwT\n",
    "    \n",
    "    if AwT < 0.5:\n",
    "        AwT_score = base_score * penalty_function_AwT(AwT, alpha)\n",
    "    else:\n",
    "        AwT_score = base_score * reward_function_AwT(AwT, beta)\n",
    "    \n",
    "    if SoE < 0.5:\n",
    "        final_score = AwT_score * penalty_function_SoE(SoE, gamma)\n",
    "    else:\n",
    "        final_score = AwT_score * reward_function_SoE(SoE, delta)\n",
    "    \n",
    "    return final_score\n",
    "\n",
    "score00 = calculate_final_score(SoE=0,AwT=0)\n",
    "score01 = calculate_final_score(SoE=0.1,AwT=0.1)\n",
    "score02 = calculate_final_score(SoE=0.2,AwT=0.2)\n",
    "score03 = calculate_final_score(SoE=0.3,AwT=0.3)\n",
    "score04 = calculate_final_score(SoE=0.4,AwT=0.4)\n",
    "score049 = calculate_final_score(SoE=0.49,AwT=0.49)\n",
    "score05 = calculate_final_score(SoE=0.5,AwT=0.5)\n",
    "score051 = calculate_final_score(SoE=0.51,AwT=0.51)\n",
    "score06 = calculate_final_score(SoE=0.2,AwT=0.2)\n",
    "score07 = calculate_final_score(SoE=0.3,AwT=0.3)\n",
    "score08 = calculate_final_score(SoE=0.4,AwT=0.4)\n",
    "score09 = calculate_final_score(SoE=0.5,AwT=0.5)\n",
    "score1 = calculate_final_score(SoE=1,AwT=1)\n",
    "\n",
    "print(score00)\n",
    "print(score04 - score03)\n",
    "print(score03 - score02)\n",
    "print(score02 - score01)\n",
    "print(\"the closer to 0 the stronger penalty\")\n",
    "print(score049, score05, score051, 'huge drop') \n",
    "print(score08 - score07)\n",
    "print(score09 - score08)\n",
    "print(\"the closer to 1 the stronger reward\")\n",
    "print(score1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kacpe\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\kacpe\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\kacpe\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 67\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m     66\u001b[0m num_models \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m---> 67\u001b[0m models \u001b[38;5;241m=\u001b[39m [train_model(train_dataloader, device) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_models)]\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_models\u001b[39m(models, test_dataloader, device):\n\u001b[0;32m     70\u001b[0m     all_predictions \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[62], line 67\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m     66\u001b[0m num_models \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m---> 67\u001b[0m models \u001b[38;5;241m=\u001b[39m [\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_models)]\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_models\u001b[39m(models, test_dataloader, device):\n\u001b[0;32m     70\u001b[0m     all_predictions \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[62], line 58\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(train_dataloader, device, epochs)\u001b[0m\n\u001b[0;32m     56\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39msqueeze(),b_labels)\n\u001b[0;32m     57\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m---> 58\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     60\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataloader)\n",
      "File \u001b[1;32mc:\\Users\\kacpe\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kacpe\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kacpe\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load your dataset\n",
    "# dataset = pd.read_csv(\"path_to_your_dataset.csv\")\n",
    "# Assuming 'text' column contains abstracts and 'score' column contains labels\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "\n",
    "# Initializing tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")\n",
    "\n",
    "# Function to prepare DataLoader\n",
    "def prepare_dataloader(data, batch_size=6, test=False):\n",
    "    inputs = tokenizer(data[\"text\"].tolist(), padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    if test:\n",
    "        randomized_train_y = randomize_scores(scores=train_y)\n",
    "        labels = torch.tensor(randomized_train_y).float()\n",
    "    labels = torch.tensor(data[[\" AwT score\", \" SoE score\"]].values).float()\n",
    "    dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'], labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader\n",
    "\n",
    "train_dataloader = prepare_dataloader(dataset)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BertForRegression(nn.Module):\n",
    "    def __init__(self, model_name, hidden_size=768):\n",
    "        super(BertForRegression, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.regressor = nn.Linear(hidden_size, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None, return_embeddings=False):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        if return_embeddings:\n",
    "            return pooled_output\n",
    "        return self.regressor(pooled_output)\n",
    "    \n",
    "\n",
    "\n",
    "def train_model(train_dataloader, device, epochs = 8):\n",
    "    model = BertForRegression(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")\n",
    "    model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5) # test value  # torch.optim.AdamW\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            b_input_ids, b_input_mask, b_labels = [item.to(device) for item in batch]\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(b_input_ids,b_input_mask)\n",
    "            loss = criterion(outputs.squeeze(),b_labels)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {avg_train_loss}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "num_models = 2\n",
    "models = [train_model(train_dataloader, device) for _ in range(num_models)]\n",
    "\n",
    "def evaluate_models(models, test_dataloader, device):\n",
    "    all_predictions = []\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for batch in test_dataloader:\n",
    "                b_input_ids, b_input_mask, _ = [item.to(device) for item in batch]\n",
    "                outputs = model(b_input_ids, b_input_mask)\n",
    "                predictions.extend(outputs.squeeze().cpu().numpy())\n",
    "        all_predictions.append(predictions)\n",
    "    \n",
    "    avg_predictions = np.mean(all_predictions, axis=0)\n",
    "    return avg_predictions\n",
    "\n",
    "test_dataloader = prepare_dataloader(dataset, batch_size=1, test=True) # Use your actual test data here\n",
    "\n",
    "avg_predictions = evaluate_models(models, test_dataloader, device)\n",
    "true_scores = dataset[[\" AwT score\", \"SoE score\"]].values\n",
    "mse = ((avg_predictions - true_scores) ** 2).mean(axis=0)\n",
    "print(f\"Average MSE: {mse}\")\n",
    "\n",
    "\n",
    "param_distributions = {\n",
    "    'alpha' : [expon(scale=1.0), uniform(0.1,1.9)],\n",
    "    'beta' : [loguniform(1e-3,1e1),uniform(0.1,1.9)],\n",
    "    'gamma' : [expon(scale=1.0),uniform(0.1,1.9)],\n",
    "    'delta' : [uniform(0.1,1.9), expon(scale=1.0)]\n",
    "}\n",
    "\n",
    "random_search=RandomizedSearchCV(\n",
    "    estimator=evaluate_hyperparameters(),\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=100,\n",
    "    scoring=make_scorer(custom_scorer, greater_is_better= False),\n",
    "    cv=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9252603  0.77776223 0.96740234 0.68590736 1.0221708  0.77937067\n",
      " 1.0072647  0.7213042  0.9108105  0.8359132  0.8974997  0.66318786\n",
      " 0.99135613 0.8875499  0.9612308  0.44953763 1.0182903  0.69183224\n",
      " 1.0016801  0.57063335 0.9987792  0.78549373 1.0043609  0.83751607\n",
      " 0.81779003 0.69042003 0.9553925  0.7575235  1.0299163  0.6484913 ]\n",
      "[[0.9  0.2 ]\n",
      " [0.9  0.5 ]\n",
      " [0.95 1.  ]\n",
      " [0.95 1.  ]\n",
      " [0.95 0.55]\n",
      " [0.95 0.55]\n",
      " [0.95 0.6 ]\n",
      " [0.95 1.  ]\n",
      " [0.95 0.4 ]\n",
      " [0.95 0.6 ]\n",
      " [0.95 0.55]\n",
      " [0.95 1.  ]\n",
      " [0.75 0.4 ]\n",
      " [0.75 0.55]\n",
      " [0.85 0.5 ]]\n"
     ]
    }
   ],
   "source": [
    "# Example data\n",
    "# X_train, y_train should be your training data\n",
    "# x_train: array of pairs (SoE, AwT)\n",
    "# y_train: corresponding true scores\n",
    "\n",
    "#random_search.fit(x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'avg_predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m true_scores \u001b[38;5;241m=\u001b[39m dataset[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m AwT score\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m SoE score\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Check the shape of avg_predictions\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of avg_predictions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_predictions\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of true_scores: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrue_scores\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# If avg_predictions is not 2D, reshape it accordingly\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'avg_predictions' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assuming true_scores is already defined correctly\n",
    "true_scores = dataset[[\" AwT score\", \" SoE score\"]].values\n",
    "\n",
    "# Check the shape of avg_predictions\n",
    "print(f\"Shape of avg_predictions: {avg_predictions.shape}\")\n",
    "print(f\"Shape of true_scores: {true_scores.shape}\")\n",
    "\n",
    "# If avg_predictions is not 2D, reshape it accordingly\n",
    "if avg_predictions.ndim == 1:\n",
    "    avg_predictions = avg_predictions.reshape(-1, 2)\n",
    "\n",
    "# Ensure avg_predictions has the same number of samples as true_scores\n",
    "if avg_predictions.shape[0] != true_scores.shape[0]:\n",
    "    raise ValueError(\"Number of samples in predictions and true scores do not match.\")\n",
    "\n",
    "mse = ((avg_predictions - true_scores) ** 2).mean(axis=0)\n",
    "print(f\"Average MSE: {mse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bs4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
